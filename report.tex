\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[12pt, a4paper]{article}
\usepackage[a4paper,margin=1in]{geometry}
\setlength\parindent{0pt}
\usepackage{mathptmx}
\usepackage{amsmath,amssymb}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{xcolor}

\author{Davide Cremonini, Artificial Intelligence, 0001137778
\\Alessia Crimaldi, Artificial Intelligence, 0001145505
\\Fabio Giordana, Artificial Intelligence, 0001145924
\\Gabriele Nanni, Artificial Intelligence, 0001146107}
\date{}
\title{Social Network Analysis of Twitter Bots}

\begin{document}
\maketitle


% Report in PDF format of at most 5000 words (also after revisions) — approximately, these correspond to 30k characters and 9–10 pages.

% CHECKLIST!!!!!!

% [ ]  It is clear how many networks you are going to analyse and their shape (monomodal, bipartite, etc.).

% [ ]  For each network in the study, it is clear what are the nodes and what are the edges (when there exists an edge between nodes) and whether these are oriented or not.

% [ ]  You apply a wide-enough range and number of measures to describe the phenomena you want to study (measures include centrality, groups, clustering, redundancy, equivalences, homophily, small-worldness, scale freedom, cohesion, connectedness, compactness, triad census, core-periphery, etc.).

% [ ]  For each measure, you explain why you apply it (what phenomenon you investigate with it) and what semantics the measure has for your network (e.g., on a transport network, you apply betweenness centrality to find its most important junctions, since it measures the extent to which a node lies on paths between other nodes).

% [ ]  The effort behind the study is appropriate for the number of students behind the project. Suggestion: projects can consider applying the same study design on different networks, to compare these (qualitatively and/or quantitatively) through the results of the same array of measures.	


\section{Introduction}
	\label{introduction}
	
	In the current historical moment, the widespread diffusion of AI systems designed to simulate human behaviour is increasingly evident. Our project aims to evaluate the effectiveness of these systems in the context of bot detection on Twitter, seeking to identify behavioural patterns that differentiate automated users (aka \textit{bots}) from human ones. To this end, we make use of Social Network Analysis techniques.


\section{Problem and Motivation}
	\label{problem-and-motivation}
	
	%\textbf{What are the problems you want to address? Why are those problems important (impact, theoretical and/or practical needs, etc.)? What are the main contributions of the project?}

    Nowadays it is clear the impact of social medias in the everyday life of people. These applications are not only a way to interact with friends or observing celebrities but they are becoming one of the most impacting form of information for all generations, from middle-aged demographics to young adults, interacting also with children most of the population extracts their knowledge through social media. The creation of automated accounts on these platforms can in fact impact the perception of people spreading false information or polarizing content, causing the clash between two sides not willing to discuss and find a middle ground. Bots can also, posting and interacting with other accounts, show a different distribution of opinions with respect to the real one, causing public opinion to be shifted and inducing entities like companies or public organizations to move in certain directions because of the feedback given by them.
\vspace{0.1cm}

    For this reasons we wanted to observe the behaviour of bots and human account to find a way to detect patterns that characterize one's relations. The focus of this project are the relations between the accounts, in the form of the "follow" action. This action is performed from one account to another signaling that the user is interested in receiving updates on the interactions of that account and it can be reciprocated.


\section{Datasets}
	\label{datasets}
	
	%\textbf{What tools did you use 1) to handle (store, manipulate) the data and 2) to compute measures on the data?}\\
	
	We conducted our analysis using the TwiBot-22 dataset \cite{twibot22}, which is publicly available on GitHub\footnote{\href{https://github.com/LuoUndergradXJTU/TwiBot-22}{https://github.com/LuoUndergradXJTU/TwiBot-22}}. 
	\vspace{0.1cm}
	
	TwiBot-22 is a comprehensive, graph-based benchmark for Twitter bot detection, featuring the largest dataset available up to date. It offers a diverse range of entities and relationships within the Twitter network, and boasts significantly improved annotation quality compared to previous datasets.
	\vspace{0.1cm}
	
	Given the extremely large size of the dataset it is impossible to work with the full dataset with our resources. For this reason preprocessing has been used to elaborate the dataset in a more efficient way.
    \vspace{0.1cm}

        \subsection{Preprocessing}
        The two files we focus on are tweets and edges. The preprocessing is needed to extract the relation \textit{follower-following} for every user to build the network and to obtain the number of tweets of each user.
        \vspace{0.1cm}
        We create chunks for edges and for tweets that is possible to store in the avaliable RAMs.
        \vspace{0.1cm}
        After this technical solution to be able to work with all data we proceed in creating the coommunities. The communities are sub-networks of people that used the same hashtag in their tweets. They will be used to check if the patterns found in the network are robust to network chances or are network specific. To test the robustness of the patterns we divided the subnetworks in different categories (large, medium and small communities) to test the results on different scales.
	
	\subsection{Adopted tools}
		The python libraries used in this project are:
        \begin{itemize}
            \item the \textit{Polars} library and the \textit{ijson} parser for the manipulation of the dataset.
            \item the \textit{NetworkX} and \textit{NetworKit} libraries to create the network and to calculate the measures on it.
            \item the \textit{seaborn}, \textit{SciPy} and \textit{scikit-learn} libraries to search possible patterns in the bots/humans behaviour.
        \end{itemize}
		
		
	\subsection{Analyzed networks}
		\begin{itemize}
			\item Large $\rightarrow$ Ukraine, Ai, Covid
			\item Medium $\rightarrow$ Nato, Deeplearning, Nftcommunity
			\item Small $\rightarrow$ Ruleoflaw, Feminist, Agenda2030
		\end{itemize}


\section{Validity and Reliability}
	\label{validity-and-reliability-not-needed-for-the-project-proposal}
	
   As discussed in the dataset official paper \cite{twibot22}, the TwiBot-22 dataset was created trying to address and mitigate known problems of previous datasets, such as poor annotation quality and low dataset scale. This led to the construction of a large social graph with real world tweets, relationships between entities and metadata. This design allows the results to be statistically relevant and it accurately reflects Twitter’s social dynamics. Moreover, the dataset benefits from a strong annotation pipeline, which guarantees reliability and consistency, and reproducibility for a wide range of bot detection and behaviour analysis studies. It is also easily accessible and freely available.
   \vspace{0.1cm}
	
    In our work, we exploited the dataset focusing only on \textit{follower-following} relationships between users. Additionally, we experimented with applying the same analytical measures to the full graph and to subgraphs based on shared hashtags, in order to introduce a topical dimension to our analysis. This approach maintains the validity of the dataset, as both the follower links and the shared hashtag activity represent authentic user behaviours and capture meaningful patterns within Twitter’s social structure. Our results are also fully reproducible and reliable, as we provide a detailed description of the preprocessing pipeline used to tailor the dataset to our specific research goals.


\section{Measures and Results}
	\label{measures}
	
	%\textbf{What measures did you apply (brief explanation of how they work)? How do they relate to the intent of the study? Why are they relevant? What is the connection among the gathered data, the applied measures, and the properties found?}\\
	
	\begin{itemize}
		\item \textbf{Centrality}:
			\begin{enumerate}
				\item Degree centrality
				\item Betweenness centrality
				\item Eigenvector centrality
				\item PageRank
				\item Reputation \cite{Beskow2020FriendBot}.
			\end{enumerate}
		\item \textbf{Groups}:
			\begin{enumerate}
				\item 
			\end{enumerate}
		\item \textbf{Clustering}:
			\begin{enumerate}
				\item 
			\end{enumerate}
		\item \textbf{Redundancy}:
			\begin{enumerate}
				\item 
			\end{enumerate}
		\item \textbf{Equivalences}:
			\begin{enumerate}
				\item Structural equivalence
				\item Regular equivalence
			\end{enumerate}
		\item \textbf{Homophily}:
			\begin{enumerate}
				\item 	
			\end{enumerate}
		\item \textbf{Small-worldness}:
			\begin{enumerate}
				\item 	
			\end{enumerate}
		\item \textbf{Scale freedom}:
			\begin{enumerate}
				\item 	
			\end{enumerate}
		\item \textbf{Cohesion}:
			\begin{enumerate}
				\item 	
			\end{enumerate}
		\item \textbf{Connectedness}:
			\begin{enumerate}
				\item 	
			\end{enumerate}
		\item \textbf{Compactness}:
			\begin{enumerate}
				\item 	
			\end{enumerate}
		\item \textbf{Triad census}:
			
		\item \textbf{Core-periphery}:
			\begin{enumerate}
				\item 	
			\end{enumerate}
	\end{itemize}
	The experiments were conducted on an NVIDIA GeForce RTX 3090 GPU (24GB VRAM).\\


\section{Conclusion}
	\label{conclusion}

	\textbf{Qualitative analysis of the quantitative findings of the study.}


\section{Critique}
	\label{critique}

	\textbf{Do you think your work solves the problem presented above? To which extent (completely, what parts)? Why? What could you have done differently to answer your research problems (e.g., gather data with additional information, build your model differently, apply alternative measures)?}\\

	The \textit{follower} and \textit{following} connections may not represent meaningful relations for bot detection, as the TwiBot-22 dataset lacks a clear separation between human and bot accounts in this aspect.


\bibliographystyle{plain}
\bibliography{references}

\end{document}
