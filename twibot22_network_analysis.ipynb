{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DYhkg3ZDBmEE"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNgl-eMPcZ4J"},"outputs":[],"source":["SNA_PROJECT_PATH = \"drive/MyDrive/SNA_Project\""]},{"cell_type":"markdown","source":["#### Installations:"],"metadata":{"id":"NXCrJtMsupXw"}},{"cell_type":"code","source":["!pip install ijson -q"],"metadata":{"id":"iJi1bAyxul5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fastparquet -q"],"metadata":{"id":"YHziawDVu13i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Imports:"],"metadata":{"id":"859glZIcuruX"}},{"cell_type":"code","source":["import pandas as pd\n","import polars as pl\n","import numpy as np"],"metadata":{"id":"tYeTOs2ivraF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import ijson\n","from typing import List, Any, Dict\n","from tqdm.notebook import tqdm\n","import fastparquet"],"metadata":{"id":"8WKQW26WvsnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5z1XkV7-7hX"},"source":["# ðŸ”Ž Exploring the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sb-DPvQkK6wf"},"outputs":[],"source":["!ls $SNA_PROJECT_PATH/TwiBot-22"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sJG3jBf_Zl4"},"outputs":[],"source":["def explore(filepath, type, max=None):\n","  if type == 'csv':\n","    dataset = pd.read_csv(filepath)\n","  elif type == 'json':\n","    dataset = pd.read_json(filepath, nrows=max)\n","  print(f\"Dataset shape is {dataset.shape}\")\n","  return dataset"]},{"cell_type":"markdown","metadata":{"id":"Ol0w2NMX_KiC"},"source":["## Users dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5t5L-fMpCsAk"},"outputs":[],"source":["explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/user.json\", 'json')"]},{"cell_type":"markdown","metadata":{"id":"0ix298yFASzb"},"source":["## Labels dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGZwalxTASK1"},"outputs":[],"source":["explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/label.csv\", 'csv')"]},{"cell_type":"markdown","metadata":{"id":"IS05sD_sAcvY"},"source":["## List dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gKwWWRAAc63"},"outputs":[],"source":["lists = explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/list.json\", 'json')\n","lists = lists.sort_values(by=['follower_count', 'member_count'], ascending=False, axis=0)\n","lists.head(100)"]},{"cell_type":"markdown","metadata":{"id":"31fJIv7XBIh9"},"source":["## Hashtag dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByLNp_fvBLRS"},"outputs":[],"source":["explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/hashtag.json\", 'json')"]},{"cell_type":"markdown","metadata":{"id":"uhs6VQeIL07g"},"source":["## Split dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hJijSVCL1E5"},"outputs":[],"source":["explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/split.csv\", 'csv')"]},{"cell_type":"markdown","metadata":{"id":"uSVC8qo8zjVT"},"source":["## Edge Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7HSgfj5znIA"},"outputs":[],"source":["edges = pl.read_csv(f\"{SNA_PROJECT_PATH}/TwiBot-22/edge.csv\", new_columns=['source', 'relation', 'target'], n_rows=66000633, skip_rows=94328880)\n","edges_hash = edges.filter(pl.col(\"relation\")==\"discuss\")\n","edges_hash.head(10000000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLx0r1pNLqAk"},"outputs":[],"source":["grouped_eh = edges_hash.group_by(\"target\").agg(pl.col(\"source\").str.join(\",\"))\n","grouped_eh.head(10)"]},{"cell_type":"markdown","metadata":{"id":"yZpoitdbTFpl"},"source":["### 'discuss' rows count:\n","\n","\n","*   0 to 10000000:\n","*   10000000 to 20000000:\n","*   20000000 to 30000000:\n","*   30000000 to 40000000:\n","*   40000000 to 50000000:\n","*   50000000 to 60000000:\n","*   60000000 to 70000000:\n","*   70000000 to 80000000:\n","*   80000000 to 90000000:\n","*   90000000 to 100000000:  5.671.120\n","*   100000000 to 110000000: 10.000.000\n","*   110000000 to 120000000: 10.000.000\n","*   120000000 to 130000000: 10.000.000\n","*   130000000 to 140000000: 10.000.000\n","*   140000000 to 150000000: 10.000.000\n","*   150000000 to 160000000: 10.000.000\n","*   160000000 to 170000000: 329.513\n","*   170000000 to end:\n","\n","There are a total of 66.000.633 \"discuss\" entries in the edge dataset, comprised between indeces 90.000.000 and 170.000.000."]},{"cell_type":"markdown","metadata":{"id":"Q2s327sKBm8z"},"source":["## Twitter dataset (split 0)"]},{"cell_type":"markdown","metadata":{"id":"EQq6mEflpRmr"},"source":["The Tweet_i datasets seem to be constitued by one giant line, without '\\n' characters. The structure is the following: <br>\n","[{json_Object_1} , ..., {json_Object_n}]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HW1NDw7f_1M8"},"outputs":[],"source":["def read_n_instances(filename, n):\n","  i=0\n","  file = open(filename, \"r\")\n","  square = file.read(1)\n","  instances = []\n","  instance = ''\n","  start = '{\"attachments\":'\n","  while(i<n):\n","    while(not instance.endswith(', {\"attachments\":', 18)):\n","      char = file.read(1)\n","      instance += char\n","    instances.append(instance[0:-17])\n","    instance=start\n","    i+=1\n","  file.close()\n","  return instances\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udOulh3sjh95"},"outputs":[],"source":["res = read_n_instances(f\"{SNA_PROJECT_PATH}/TwiBot-22/tweet_0.json\", 100)\n","for i in res:\n","  print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SemnKS1FNJrt"},"outputs":[],"source":["df_inter = pd.DataFrame(res)\n","#df_inter.columns = ['attachments', 'author_id', 'context_annotations', 'conversation_id', 'created_at', 'entities', 'geo', 'id', 'in_reply_to_user_id', 'lang', 'possibly_sensitive', 'public_metrics', 'referenced_tweets', 'reply_sttings', 'source', 'text', 'withheld']\n","df_inter.columns = ['json_element']\n","\n","import json\n","df_inter['json_element'].apply(json.loads)\n","\n","df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', None)\n","df_final=df_final.sort_values(by='entities.hashtags', ascending=False , axis=0, key=lambda col: [len(i) for i in col])\n","df_final.head(10)"]},{"cell_type":"markdown","metadata":{"id":"Xs2Bkipxo9r-"},"source":["# âœ‚ï¸Ž Splitting into chunks\n","https://github.com/LuoUndergradXJTU/TwiBot-22/issues/17"]},{"cell_type":"markdown","metadata":{"id":"atCFoTav25nc"},"source":["## Parsing Tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFbmGAUxvRi2"},"outputs":[],"source":["class TweetsParser:\n","  \"\"\"\n","  This class parses large tweet JSON files, extracts relevant information,\n","  and saves them into smaller Parquet files for efficient processing.\n","  \"\"\"\n","  def __init__(self, tweets_path: str, batch: int=0, chunk_size: int=1000000) -> None:\n","    # tweets_path: path of the large tweet chunk\n","    # chunk_size: size of the mini chunks\n","    # batch: offset of the chunk indices\n","    self.tweets_path = tweets_path\n","    self.chunk_size = chunk_size\n","    self.batch = batch\n","\n","  def change_tweets_path(self, new_tweets_path):\n","    self.tweets_path = new_tweets_path\n","\n","  def _extract_hashtags(self, entity: Dict) -> List:\n","    if not entity or 'hashtags' not in entity:\n","        return []\n","    return entity.get('hashtags', [])\n","\n","  def _save_mini_chunk(self, records: List[Any], chunk_number: int, output_dir: str=f\"{SNA_PROJECT_PATH}/tweet_chunks\"):\n","      os.makedirs(output_dir, exist_ok=True)\n","\n","      df = pd.DataFrame(records)\n","\n","      df['hashtags'] = df['entities'].apply(self._extract_hashtags)\n","      cols_to_drop = ['attachments', 'context_annotations', 'conversation_id', 'created_at', 'geo', 'id', 'lang', 'possibly_sensitive', 'referenced_tweets', 'reply_settings', 'source', 'text', 'withheld', 'entities', 'public_metrics']\n","      df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n","\n","      output_path = os.path.join(output_dir, f\"tweet_chunk_{chunk_number}.parquet\")\n","      df.to_parquet(output_path, compression='snappy', index=False)\n","\n","      print(f\"Saved chunk {chunk_number} with {len(records)} records to {output_path}\")\n","\n","  def parse(self):\n","      with open(self.tweets_path, 'r') as f:\n","          data = ijson.items(f, 'item')\n","\n","          records = []\n","          chunk_count = 10*self.batch\n","          for item in tqdm(data, desc=\"Parsing tweets\", unit=\" tweets\"):\n","              records.append(item)\n","              if len(records) >= self.chunk_size:\n","                  self._save_mini_chunk(records, chunk_count)\n","                  chunk_count += 1\n","                  records = []\n","          # check for remaining tweets\n","          if records:\n","            self._save_mini_chunk(records, chunk_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpHRoy4s_qTq"},"outputs":[],"source":["for i in range(4,9):\n","  parser = TweetsParser(f\"{SNA_PROJECT_PATH}/TwiBot-22/tweet_{i}.json\", i)\n","  parser.parse()"]},{"cell_type":"markdown","metadata":{"id":"9tWonjB_sI84"},"source":["### Tweet chunks' dimension check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zx_dMc8sEov"},"outputs":[],"source":["tweets_count = 0\n","for i in range(89):\n","  tweet_chunk_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/tweet_chunks/tweet_chunk_{i}.parquet\")\n","  tweet_chunk_shape = tweet_chunk_df.shape\n","  print(f\"Shape of file tweet_chunk_{i}: {tweet_chunk_shape}\")\n","  tweets_count += tweet_chunk_shape[0]\n","print(f\"# Tweet: {tweets_count}\")"]},{"cell_type":"markdown","metadata":{"id":"m3d4OBAHyvka"},"source":["### Tweet first chunk visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdwchvnD9MVG"},"outputs":[],"source":["tweet_chunk_0_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/tweet_chunks/tweet_chunk_0.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVWNlRjZw44j"},"outputs":[],"source":["pd.set_option('display.max_colwidth', None)\n","tweet_chunk_0_df"]},{"cell_type":"markdown","metadata":{"id":"m4yzVMaRrlpC"},"source":["## Parsing Edges"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swKwjqMBrlU_"},"outputs":[],"source":["class EdgeParser():\n","    \"\"\"\n","    This class parses a large edge CSV file, filters edges based on specified relations,\n","    and saves the filtered edges into smaller Parquet files for efficient processing.\n","    \"\"\"\n","    def __init__(self, edges_path: str, relations, output_dir, chunk_size: int=500000):\n","      self.edges_path = edges_path\n","      self.relations = relations\n","      self.output_dir = output_dir\n","      self.chunk_size = chunk_size\n","\n","    def __save_edges__(self, edges, chunk_count):\n","      os.makedirs(self.output_dir, exist_ok=True)\n","      df = pd.concat(edges, ignore_index=True)\n","      output_path = os.path.join(self.output_dir, f\"edge_chunk_{chunk_count}.parquet\")\n","      df.to_parquet(output_path, compression='snappy', index=False)\n","      print(f\"Saved chunk {chunk_count} with {sum(len(df) for df in edges)} records to {output_path}\")\n","\n","    def parse(self):\n","      filtered_edges = []\n","      chunk_count = 0\n","      for chunk in pd.read_csv(self.edges_path, usecols=['source_id', 'relation', 'target_id'], chunksize=self.chunk_size):\n","        filtered_chunk = chunk[chunk[\"relation\"].isin(self.relations)]\n","        filtered_edges.append(filtered_chunk)\n","        if sum(len(df) for df in filtered_edges) >= self.chunk_size:\n","          self.__save_edges__(filtered_edges, chunk_count)\n","          chunk_count += 1\n","          filtered_edges = []\n","      if len(filtered_edges) > 0:\n","          self.__save_edges__(filtered_edges, chunk_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cl-xGUYvwNC6"},"outputs":[],"source":["edge_parser = EdgeParser(f\"{SNA_PROJECT_PATH}/TwiBot-22/edge.csv\", set([\"followers\", \"following\"]), f\"{SNA_PROJECT_PATH}/edge_chunks\")\n","edge_parser.parse()"]},{"cell_type":"markdown","metadata":{"id":"cMgvKp4-x-Si"},"source":["### Edge chunks' dimension check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSixutcSx9bB"},"outputs":[],"source":["edges_count = 0\n","for i in range(8):\n","  edge_chunk_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/edge_chunks/edge_chunk_{i}.parquet\")\n","  edge_chunk_shape = edge_chunk_df.shape\n","  print(f\"Shape of file edge_chunk_{i}: {edge_chunk_shape}\")\n","  edges_count += edge_chunk_shape[0]\n","print(f\"# Edge (followers & following): {edges_count}\")"]},{"cell_type":"markdown","metadata":{"id":"nIvF2iJV0Jsv"},"source":["### Edge last chunk visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6XejJz18V_k"},"outputs":[],"source":["edge_chunk_7_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/edge_chunks/edge_chunk_7.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdVIwunM8uD-"},"outputs":[],"source":["edge_chunk_7_df"]}],"metadata":{"colab":{"collapsed_sections":["cMgvKp4-x-Si"],"provenance":[{"file_id":"1LHSvAGTg_Wn-0Zw3fImDAEYcIPI3XxLW","timestamp":1744788480472},{"file_id":"1p1FhtVGyCPP_3Gz_0yA1LwuRB40KpZJU","timestamp":1744705668244},{"file_id":"1LE9qdYu9ixECxhJNURVcomJ6t0Bjsqvw","timestamp":1744212016316},{"file_id":"1qXFerMWRLhpA4oPQ6tycChTtwZwlGCz4","timestamp":1743411181081},{"file_id":"1bRDVpqDyltQvCUOzEmlcydCMLpLeoEUG","timestamp":1743263936683}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}