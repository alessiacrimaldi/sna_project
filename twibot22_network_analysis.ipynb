{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DYhkg3ZDBmEE"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNgl-eMPcZ4J"},"outputs":[],"source":["SNA_PROJECT_PATH = \"drive/MyDrive/SNA_Project\""]},{"cell_type":"markdown","source":["#### Installations:"],"metadata":{"id":"NXCrJtMsupXw"}},{"cell_type":"code","source":["!pip install ijson -q"],"metadata":{"id":"iJi1bAyxul5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fastparquet -q"],"metadata":{"id":"YHziawDVu13i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install networkit"],"metadata":{"id":"j73JD3Iguzm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gravis"],"metadata":{"id":"vxduxw32AnLd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Imports:"],"metadata":{"id":"859glZIcuruX"}},{"cell_type":"code","source":["# Data analysis and manipulation libraries\n","import pandas as pd\n","import polars as pl\n","import numpy as np"],"metadata":{"id":"tYeTOs2ivraF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# File/OS handling, JSON parsing, progress display and Parquet file operations libraries\n","import os\n","import ijson\n","from typing import List, Any, Dict\n","from tqdm.notebook import tqdm\n","import fastparquet"],"metadata":{"id":"8WKQW26WvsnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Network analysis and graph manipulation libraries\n","import networkx as nx\n","from networkx import subgraph_view\n","import networkit as nk"],"metadata":{"id":"mEEGiLDnvtzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Graphs visualization libraries\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","import matplotlib.cm as cm\n","import gravis as gv"],"metadata":{"id":"4xgsL_14uZWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Machine learning libraries\n","import scipy as spy\n","import seaborn as sns\n","from sklearn.ensemble import IsolationForest\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.cluster import DBSCAN\n","from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score\n","import statsmodels.api as sm"],"metadata":{"id":"V1g-vAgpB8CT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Multiprocessing for parallel tasks\n","import multiprocessing\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/SNA_Project\")\n","from metrics import *"],"metadata":{"id":"4PUqp_5TmiiC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5z1XkV7-7hX"},"source":["# 🔎 Exploring the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sb-DPvQkK6wf"},"outputs":[],"source":["!ls $SNA_PROJECT_PATH/TwiBot-22"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sJG3jBf_Zl4"},"outputs":[],"source":["def explore(filepath, type, max=None):\n","  if type == 'csv':\n","    dataset = pd.read_csv(filepath)\n","  elif type == 'json':\n","    dataset = pd.read_json(filepath, nrows=max)\n","  print(f\"Dataset shape is {dataset.shape}\")\n","  return dataset"]},{"cell_type":"markdown","metadata":{"id":"Ol0w2NMX_KiC"},"source":["## Users dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5t5L-fMpCsAk"},"outputs":[],"source":["explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/user.json\", 'json')"]},{"cell_type":"markdown","metadata":{"id":"0ix298yFASzb"},"source":["## Labels dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGZwalxTASK1"},"outputs":[],"source":["explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/label.csv\", 'csv')"]},{"cell_type":"markdown","metadata":{"id":"IS05sD_sAcvY"},"source":["## List dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gKwWWRAAc63"},"outputs":[],"source":["lists = explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/list.json\", 'json')\n","lists = lists.sort_values(by=['follower_count', 'member_count'], ascending=False, axis=0)\n","lists.head(100)"]},{"cell_type":"markdown","metadata":{"id":"31fJIv7XBIh9"},"source":["## Hashtag dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByLNp_fvBLRS"},"outputs":[],"source":["explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/hashtag.json\", 'json')"]},{"cell_type":"markdown","metadata":{"id":"uhs6VQeIL07g"},"source":["## Split dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hJijSVCL1E5"},"outputs":[],"source":["explore(f\"{SNA_PROJECT_PATH}/TwiBot-22/split.csv\", 'csv')"]},{"cell_type":"markdown","metadata":{"id":"uSVC8qo8zjVT"},"source":["## Edge Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7HSgfj5znIA"},"outputs":[],"source":["edges = pl.read_csv(f\"{SNA_PROJECT_PATH}/TwiBot-22/edge.csv\", new_columns=['source', 'relation', 'target'], n_rows=66000633, skip_rows=94328880)\n","edges_hash = edges.filter(pl.col(\"relation\")==\"discuss\")\n","edges_hash.head(10000000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLx0r1pNLqAk"},"outputs":[],"source":["grouped_eh = edges_hash.group_by(\"target\").agg(pl.col(\"source\").str.join(\",\"))\n","grouped_eh.head(10)"]},{"cell_type":"markdown","metadata":{"id":"yZpoitdbTFpl"},"source":["### 'discuss' rows count:\n","\n","\n","*   0 to 10000000:\n","*   10000000 to 20000000:\n","*   20000000 to 30000000:\n","*   30000000 to 40000000:\n","*   40000000 to 50000000:\n","*   50000000 to 60000000:\n","*   60000000 to 70000000:\n","*   70000000 to 80000000:\n","*   80000000 to 90000000:\n","*   90000000 to 100000000:  5.671.120\n","*   100000000 to 110000000: 10.000.000\n","*   110000000 to 120000000: 10.000.000\n","*   120000000 to 130000000: 10.000.000\n","*   130000000 to 140000000: 10.000.000\n","*   140000000 to 150000000: 10.000.000\n","*   150000000 to 160000000: 10.000.000\n","*   160000000 to 170000000: 329.513\n","*   170000000 to end:\n","\n","There are a total of 66.000.633 \"discuss\" entries in the edge dataset, comprised between indeces 90.000.000 and 170.000.000."]},{"cell_type":"markdown","metadata":{"id":"Q2s327sKBm8z"},"source":["## Twitter dataset (split 0)"]},{"cell_type":"markdown","metadata":{"id":"EQq6mEflpRmr"},"source":["The Tweet_i datasets seem to be constitued by one giant line, without '\\n' characters. The structure is the following: <br>\n","[{json_Object_1} , ..., {json_Object_n}]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HW1NDw7f_1M8"},"outputs":[],"source":["def read_n_instances(filename, n):\n","  i=0\n","  file = open(filename, \"r\")\n","  square = file.read(1)\n","  instances = []\n","  instance = ''\n","  start = '{\"attachments\":'\n","  while(i<n):\n","    while(not instance.endswith(', {\"attachments\":', 18)):\n","      char = file.read(1)\n","      instance += char\n","    instances.append(instance[0:-17])\n","    instance=start\n","    i+=1\n","  file.close()\n","  return instances\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udOulh3sjh95"},"outputs":[],"source":["res = read_n_instances(f\"{SNA_PROJECT_PATH}/TwiBot-22/tweet_0.json\", 100)\n","for i in res:\n","  print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SemnKS1FNJrt"},"outputs":[],"source":["df_inter = pd.DataFrame(res)\n","#df_inter.columns = ['attachments', 'author_id', 'context_annotations', 'conversation_id', 'created_at', 'entities', 'geo', 'id', 'in_reply_to_user_id', 'lang', 'possibly_sensitive', 'public_metrics', 'referenced_tweets', 'reply_sttings', 'source', 'text', 'withheld']\n","df_inter.columns = ['json_element']\n","\n","import json\n","df_inter['json_element'].apply(json.loads)\n","\n","df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', None)\n","df_final=df_final.sort_values(by='entities.hashtags', ascending=False , axis=0, key=lambda col: [len(i) for i in col])\n","df_final.head(10)"]},{"cell_type":"markdown","metadata":{"id":"Xs2Bkipxo9r-"},"source":["# ✂︎ Splitting into chunks\n","https://github.com/LuoUndergradXJTU/TwiBot-22/issues/17"]},{"cell_type":"markdown","metadata":{"id":"atCFoTav25nc"},"source":["## Parsing Tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFbmGAUxvRi2"},"outputs":[],"source":["class TweetsParser:\n","  \"\"\"\n","  This class parses large tweet JSON files, extracts relevant information,\n","  and saves them into smaller Parquet files for efficient processing.\n","  \"\"\"\n","  def __init__(self, tweets_path: str, batch: int=0, chunk_size: int=1000000) -> None:\n","    # tweets_path: path of the large tweet chunk\n","    # chunk_size: size of the mini chunks\n","    # batch: offset of the chunk indices\n","    self.tweets_path = tweets_path\n","    self.chunk_size = chunk_size\n","    self.batch = batch\n","\n","  def change_tweets_path(self, new_tweets_path):\n","    self.tweets_path = new_tweets_path\n","\n","  def _extract_hashtags(self, entity: Dict) -> List:\n","    if not entity or 'hashtags' not in entity:\n","        return []\n","    return entity.get('hashtags', [])\n","\n","  def _save_mini_chunk(self, records: List[Any], chunk_number: int, output_dir: str=f\"{SNA_PROJECT_PATH}/tweet_chunks\"):\n","      os.makedirs(output_dir, exist_ok=True)\n","\n","      df = pd.DataFrame(records)\n","\n","      df['hashtags'] = df['entities'].apply(self._extract_hashtags)\n","      cols_to_drop = ['attachments', 'context_annotations', 'conversation_id', 'created_at', 'geo', 'id', 'lang', 'possibly_sensitive', 'referenced_tweets', 'reply_settings', 'source', 'text', 'withheld', 'entities', 'public_metrics']\n","      df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n","\n","      output_path = os.path.join(output_dir, f\"tweet_chunk_{chunk_number}.parquet\")\n","      df.to_parquet(output_path, compression='snappy', index=False)\n","\n","      print(f\"Saved chunk {chunk_number} with {len(records)} records to {output_path}\")\n","\n","  def parse(self):\n","      with open(self.tweets_path, 'r') as f:\n","          data = ijson.items(f, 'item')\n","\n","          records = []\n","          chunk_count = 10*self.batch\n","          for item in tqdm(data, desc=\"Parsing tweets\", unit=\" tweets\"):\n","              records.append(item)\n","              if len(records) >= self.chunk_size:\n","                  self._save_mini_chunk(records, chunk_count)\n","                  chunk_count += 1\n","                  records = []\n","          # check for remaining tweets\n","          if records:\n","            self._save_mini_chunk(records, chunk_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpHRoy4s_qTq"},"outputs":[],"source":["for i in range(4,9):\n","  parser = TweetsParser(f\"{SNA_PROJECT_PATH}/TwiBot-22/tweet_{i}.json\", i)\n","  parser.parse()"]},{"cell_type":"markdown","metadata":{"id":"9tWonjB_sI84"},"source":["### Tweet chunks' dimension check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zx_dMc8sEov"},"outputs":[],"source":["tweets_count = 0\n","for i in range(89):\n","  tweet_chunk_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/tweet_chunks/tweet_chunk_{i}.parquet\")\n","  tweet_chunk_shape = tweet_chunk_df.shape\n","  print(f\"Shape of file tweet_chunk_{i}: {tweet_chunk_shape}\")\n","  tweets_count += tweet_chunk_shape[0]\n","print(f\"# Tweet: {tweets_count}\")"]},{"cell_type":"markdown","metadata":{"id":"m3d4OBAHyvka"},"source":["### Tweet first chunk visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdwchvnD9MVG"},"outputs":[],"source":["tweet_chunk_0_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/tweet_chunks/tweet_chunk_0.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVWNlRjZw44j"},"outputs":[],"source":["pd.set_option('display.max_colwidth', None)\n","tweet_chunk_0_df"]},{"cell_type":"markdown","metadata":{"id":"m4yzVMaRrlpC"},"source":["## Parsing Edges"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swKwjqMBrlU_"},"outputs":[],"source":["class EdgeParser():\n","    \"\"\"\n","    This class parses a large edge CSV file, filters edges based on specified relations,\n","    and saves the filtered edges into smaller Parquet files for efficient processing.\n","    \"\"\"\n","    def __init__(self, edges_path: str, relations, output_dir, chunk_size: int=500000):\n","      self.edges_path = edges_path\n","      self.relations = relations\n","      self.output_dir = output_dir\n","      self.chunk_size = chunk_size\n","\n","    def __save_edges__(self, edges, chunk_count):\n","      os.makedirs(self.output_dir, exist_ok=True)\n","      df = pd.concat(edges, ignore_index=True)\n","      output_path = os.path.join(self.output_dir, f\"edge_chunk_{chunk_count}.parquet\")\n","      df.to_parquet(output_path, compression='snappy', index=False)\n","      print(f\"Saved chunk {chunk_count} with {sum(len(df) for df in edges)} records to {output_path}\")\n","\n","    def parse(self):\n","      filtered_edges = []\n","      chunk_count = 0\n","      for chunk in pd.read_csv(self.edges_path, usecols=['source_id', 'relation', 'target_id'], chunksize=self.chunk_size):\n","        filtered_chunk = chunk[chunk[\"relation\"].isin(self.relations)]\n","        filtered_edges.append(filtered_chunk)\n","        if sum(len(df) for df in filtered_edges) >= self.chunk_size:\n","          self.__save_edges__(filtered_edges, chunk_count)\n","          chunk_count += 1\n","          filtered_edges = []\n","      if len(filtered_edges) > 0:\n","          self.__save_edges__(filtered_edges, chunk_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cl-xGUYvwNC6"},"outputs":[],"source":["edge_parser = EdgeParser(f\"{SNA_PROJECT_PATH}/TwiBot-22/edge.csv\", set([\"followers\", \"following\"]), f\"{SNA_PROJECT_PATH}/edge_chunks\")\n","edge_parser.parse()"]},{"cell_type":"markdown","metadata":{"id":"cMgvKp4-x-Si"},"source":["### Edge chunks' dimension check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSixutcSx9bB"},"outputs":[],"source":["edges_count = 0\n","for i in range(8):\n","  edge_chunk_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/edge_chunks/edge_chunk_{i}.parquet\")\n","  edge_chunk_shape = edge_chunk_df.shape\n","  print(f\"Shape of file edge_chunk_{i}: {edge_chunk_shape}\")\n","  edges_count += edge_chunk_shape[0]\n","print(f\"# Edge (followers & following): {edges_count}\")"]},{"cell_type":"markdown","metadata":{"id":"nIvF2iJV0Jsv"},"source":["### Edge last chunk visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6XejJz18V_k"},"outputs":[],"source":["edge_chunk_7_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/edge_chunks/edge_chunk_7.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdVIwunM8uD-"},"outputs":[],"source":["edge_chunk_7_df"]},{"cell_type":"markdown","metadata":{"id":"6TwaTFS90-lw"},"source":["# #️⃣ Creating Hashtag Community"]},{"cell_type":"code","source":["unique_users = False"],"metadata":{"id":"2H3EOxGhnDEp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Two possible types of dictionaries:\n","\n","- **hashtag_users:** contains only unique users for each hashtag, regardless of how many times they used it\n","\n","  $\\rightarrow$ `unique_users = True`\n","\n","- **hashtag_users_non_unique:** contains all instances of users using a hashtag, even if a user uses the same hashtag multiple times\n","\n","  $\\rightarrow$ `unique_users = False`"],"metadata":{"id":"-lsg4-DImbk4"}},{"cell_type":"markdown","source":["## Creating hashtag-user dictionary"],"metadata":{"id":"6SvDn_jaiJ6c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbmACWcTJ1Ur"},"outputs":[],"source":["def user_hashtag(df, dictionary, unique_users=True):\n","  \"\"\"\n","  This method iterates through a DataFrame of tweets and creates a dictionary\n","  that maps hashtags to the users who have used them.\n","\n","  Args:\n","    df: DataFrame of tweets.\n","    dictionary: Dictionary to populate.\n","\n","  Returns:\n","    None. The dictionary is updated in-place.\n","  \"\"\"\n","  for tweet in tqdm(df.iterrows(), desc=\"Parsing tweets\", unit=\"tweets\"):\n","    for hash_dict in tweet[1]['hashtags']:\n","        try:\n","          tag = hash_dict['tag']\n","        except KeyError:\n","          text = hash_dict['text']\n","          tag = None\n","\n","        try:\n","          text = hash_dict['text']\n","        except KeyError:\n","          tag = hash_dict['tag']\n","          text = None\n","\n","        hashtag = tag if tag is not None else text\n","\n","        user_container = dictionary.get(hashtag, set() if unique_users else list())\n","        user_action = user_container.add if unique_users else user_container.append\n","        user_action(f\"u{tweet[1]['author_id']}\")\n","\n","        dictionary.update({hashtag: user_container})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ikz8L_9zNzvh"},"outputs":[],"source":["discussions = dict()\n","for i in range(89):\n","    print(f\"Parsing chunk {i+1}/89:\")\n","    tweet_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/tweet_chunks/tweet_chunk_{i}.parquet\")\n","    user_hashtag(tweet_df, discussions, unique_users)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiA8oJuhOQDs"},"outputs":[],"source":["# Identifying and printing popular hashtags based on the number of users associated with them\n","for key in discussions.keys():\n","  if len(discussions[key]) > 10:\n","    print(f\"{key}: {len(discussions[key])}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCxfG2IqTw5v"},"outputs":[],"source":["# Displaying the full content of a specific tweet from the dataset\n","tweet_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/tweet_chunks/tweet_chunk_0.parquet\")\n","pd.set_option('display.max_colwidth', None)\n","print(tweet_df.iloc()[817766])"]},{"cell_type":"markdown","source":["## Saving dictionary to Parquet file"],"metadata":{"id":"YtCUSXafmuVl"}},{"cell_type":"code","source":["# Chunk size to process at a time\n","chunk_size = 1_000_000\n","\n","# Convert to Lists if unique_users is False, otherwise leave as sets\n","if not unique_users:\n","    dict_as_lists = {k: list(v) for k, v in discussions.items()}\n","else:\n","    dict_as_lists = discussions\n","\n","# Determine the filename based on unique_users\n","filename = f\"{SNA_PROJECT_PATH}/hashtag_users\"\n","filename += \"_non_unique.parquet\" if not unique_users else \".parquet\"\n","\n","# Write in append mode\n","for i in range(0, len(dict_as_lists.keys()), chunk_size):\n","    # Handle chunking differently based on unique_users\n","    if not unique_users:\n","        chunk = {k: v[i:i + chunk_size] for k, v in dict_as_lists.items()}\n","        df = pd.DataFrame([(k, v) for k, values in chunk.items() for v in values], columns=[\"Hashtag\", \"UserID\"])  # Convert only a small part to DataFrame\n","    else:\n","        chunk_keys = list(dict_as_lists.keys())[i:i + chunk_size]\n","        chunk = {k: list(v) for k, v in dict_as_lists.items() if k in chunk_keys}  # Convert to list for DataFrame\n","        df = pd.DataFrame([(k, v) for k, values in chunk.items() for v in values], columns=[\"Hashtag\", \"UserID\"])\n","\n","    # Write to Parquet file\n","    if i == 0:\n","        fastparquet.write(filename, df)\n","    else:\n","        fastparquet.write(filename, df, append=True)"],"metadata":{"id":"A2FnLtCiNDtA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reading dictionary from Parquet file"],"metadata":{"id":"4ANMOBppjwEG"}},{"cell_type":"code","source":["# Loading and previewing the hashtag-user mappings data from the Parquet file\n","user_hashtag_unique_parquet = pd.read_parquet(f\"{SNA_PROJECT_PATH}/hashtag_users.parquet\")\n","user_hashtag_unique_parquet.head()"],"metadata":{"id":"lKDkPSf5NHza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading and previewing the hashtag-user (non unique) mappings data from the Parquet file\n","user_hashtag_non_unique_parquet = pd.read_parquet(f\"{SNA_PROJECT_PATH}/hashtag_users_non_unique.parquet\")\n","user_hashtag_non_unique_parquet.head()"],"metadata":{"id":"NJ2_G3kdWnJ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TMXdsw9a3KzL"},"source":["# 🕸️ Network Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6uC0ZA84KGY5"},"outputs":[],"source":["user_hashtag_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/hashtag_users.parquet\")\n","user_hashtag_df.head(10)"]},{"cell_type":"code","source":["hashtag_counts_df = user_hashtag_df.value_counts().reset_index(name='Occurrences')\n","hashtag_counts_df.sort_values(by='Occurrences', ascending=False).head()"],"metadata":{"id":"REXhgnC0ggQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2wwF1q6vYjT"},"outputs":[],"source":["communities = user_hashtag_df['Hashtag'].str.lower().value_counts()"]},{"cell_type":"markdown","metadata":{"id":"mQHskSsSLWm7"},"source":["## Large Communities (> 10.000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2LJWqJGlLUk4"},"outputs":[],"source":["large_communities = [(i,communities[i]) for i in communities.index if communities[i]>10000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5ocXQ3SNKAY"},"outputs":[],"source":["print(large_communities)"]},{"cell_type":"markdown","metadata":{"id":"CUKmPLIoLZ75"},"source":["## Medium Communities (1.000 - 10.000)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7JAG1G9LUYd"},"outputs":[],"source":["medium_communities = [(i,communities[i]) for i in communities.index if communities[i]>1000 and communities[i]<=10000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bPxNvSgNiBf"},"outputs":[],"source":["print(medium_communities)"]},{"cell_type":"markdown","metadata":{"id":"doOsw-pNLcAM"},"source":["## Small Communities (< 1.000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlwmPaB-LMlb"},"outputs":[],"source":["small_communities = [(i,communities[i]) for i in communities.index if communities[i]>10 and communities[i]<=1000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0SNegvJNrUp"},"outputs":[],"source":["print(small_communities[0:20])"]},{"cell_type":"markdown","source":["## Network selection"],"metadata":{"id":"LKDt_KIynjlF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2XOc0J0GLlP"},"outputs":[],"source":["community_name = \"ukraine\"\n","#community = hashtag_counts_df[hashtag_counts_df['Hashtag'].str.contains(community_name, case=False)]['UserID'].to_list()  # Non-unique\n","community = user_hashtag_df[user_hashtag_df['Hashtag'].str.lower()==community_name]['UserID'].to_list()                    # Unique\n","print(len(community))\n","print(community[:20])"]},{"cell_type":"code","source":["print(len(set(community)))"],"metadata":{"id":"VUUY6MRY1LCE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter rows where the hashtag contains the community name (case-insensitive)\n","community_hashtags = hashtag_counts_df[hashtag_counts_df['Hashtag'].str.contains(community_name, case=False)]\n","\n","# Group by UserID and sum the Occurrences\n","community_df = community_hashtags.groupby('UserID')['Occurrences'].sum().reset_index()\n","\n","community_df.sort_values('Occurrences', ascending=False).head(10)"],"metadata":{"id":"OhnSv4ZnuWja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(community_df['UserID']))"],"metadata":{"id":"J4QlmNpf0C_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQzyr3PrMQEO"},"outputs":[],"source":["def create_df_network(df, users):\n","  return df[(df[\"source_id\"].isin(users)) & (df[\"target_id\"].isin(users))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"93rW_bPr7t5P"},"outputs":[],"source":["df_list = []\n","for k in range(8):\n","  chunk_edge_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/edge_chunks/edge_chunk_{k}.parquet\")\n","  df = create_df_network(chunk_edge_df, community)\n","  df_list.append(df)\n","\n","final_df = pd.concat(df_list, ignore_index=True)\n","print(final_df.shape)\n","final_df.head(100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqVrKLrpPWQK"},"outputs":[],"source":["unique_sources = final_df['source_id'].unique()\n","unique_targets = final_df['target_id'].unique()\n","node_list = pd.concat([pd.Series(unique_sources), pd.Series(unique_targets)], ignore_index=True).unique()\n","print(len(node_list))"]},{"cell_type":"markdown","metadata":{"id":"JegudI9GROut"},"source":["# 🛠️ Full Network Construction\n","\n","The final `full_graph` is a directed graph where:\n","\n","- **Nodes:** Represent users within the selected community.\n","- **Edges:** Represent follower and following relationships between those users.\n","- **Node attributes:** Include the user's label ('human' or 'bot'), color (based on label), number of posts (N_posts), and calculated network measures (added in later steps).\n","\n","In essence, we construct a network graph representing the interactions and relationships between users within a specific community on Twitter, based on their follower/following connections and labeled as 'human' or 'bot' for further analysis."]},{"cell_type":"markdown","metadata":{"id":"xGbZhvoondCl"},"source":["## 1. Defining dictionaries for user labels and color mappings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzS7OPppeKFo"},"outputs":[],"source":["labels = pd.read_csv(f\"{SNA_PROJECT_PATH}/TwiBot-22/label.csv\")\n","labels.head()\n","colormap = {'human':'green', 'bot':'red'}\n","color_df = labels.replace(colormap)\n","label_dict = labels.set_index('id')['label'].to_dict()\n","color_dict = color_df.set_index('id')['label'].to_dict()"]},{"cell_type":"markdown","metadata":{"id":"hlpRlNr2njhL"},"source":["## 2. Creating following graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVBn52AHRU-K"},"outputs":[],"source":["following_df = final_df[final_df['relation']=='following']\n","following_graph = nx.from_pandas_edgelist(following_df, 'source_id', 'target_id', create_using=nx.DiGraph())"]},{"cell_type":"markdown","metadata":{"id":"A7cNn7CvoGNa"},"source":["## 3. Creating followers graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yePH79lgSBTt"},"outputs":[],"source":["followers_df = final_df[final_df['relation']=='followers']\n","#followers_graph = nx.from_pandas_edgelist(followers_df, 'source_id', 'target_id', create_using=nx.DiGraph())\n","followers_graph = nx.from_pandas_edgelist(followers_df, 'target_id', 'source_id', create_using=nx.DiGraph())  # Reversing the follower relationship, this way every arc in the graph goes from follower to followed"]},{"cell_type":"markdown","metadata":{"id":"GCLtHvgUn_S4"},"source":["## 4. Combining the following and follower graphs"]},{"cell_type":"markdown","source":["##### Unique"],"metadata":{"id":"oXMC5wUcet27"}},{"cell_type":"code","source":["nx.set_node_attributes(following_graph, label_dict, 'label')\n","nx.set_node_attributes(following_graph, color_dict, 'color')"],"metadata":{"id":"GCRi2gnbfUH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nx.set_node_attributes(followers_graph, label_dict, 'label')\n","nx.set_node_attributes(followers_graph, color_dict, 'color')"],"metadata":{"id":"WkFsvB3se7dX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_graph = nx.compose(following_graph, followers_graph)"],"metadata":{"id":"MQYjoOxmeqh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nodes = full_graph.nodes()\n","edge_dict = {}\n","for e in full_graph.edges():\n","  start_label = nodes[e[0]]['label']\n","  end_label = nodes[e[1]]['label']\n","  edge_dict[e] = start_label + '_' + end_label\n","\n","nx.set_edge_attributes(full_graph, edge_dict, 'edge_label')"],"metadata":{"id":"CUIJLtpvexA-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Nodes: {len(full_graph.nodes())}\")\n","print(f\"Edges: {len(full_graph.edges())}\")"],"metadata":{"id":"1wvuIiZ8exQp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Non-unique"],"metadata":{"id":"ti8gpyd0erKi"}},{"cell_type":"code","source":["community_df.set_index('UserID')\n","community_dict = {}\n","for u in range(len(community_df)):\n","  community_dict[community_df.loc[u]['UserID']] = community_df.loc[u]['Occurrences']"],"metadata":{"id":"0bl4N4bH2LPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m17TRJhgn8ts"},"outputs":[],"source":["full_graph = nx.compose(following_graph, followers_graph)\n","full_graph.add_nodes_from(community)\n","nx.set_node_attributes(full_graph, community_dict, 'N_posts')\n","nx.set_node_attributes(full_graph, label_dict, 'label')\n","nx.set_node_attributes(full_graph, color_dict, 'color')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXfA2_Gpp-qD"},"outputs":[],"source":["nodes = full_graph.nodes()\n","edge_dict = {}\n","for e in full_graph.edges():\n","  start_label = nodes[e[0]]['label']\n","  end_label = nodes[e[1]]['label']\n","  edge_dict[e] = start_label + '_' + end_label\n","\n","nx.set_edge_attributes(full_graph, edge_dict, 'edge_label')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7diKGUQhnyMM"},"outputs":[],"source":["print(f\"Nodes: {len(full_graph.nodes())}\")\n","print(f\"Edges: {len(full_graph.edges())}\")\n","print(full_graph)"]},{"cell_type":"markdown","metadata":{"id":"LrTOxoPdoNpV"},"source":["## 5. Plotting the full graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLudvwuSS7fp"},"outputs":[],"source":["plt.figure(figsize=(30,30))\n","colors = [colormap[full_graph.nodes[node]['label']] for node in list(full_graph.nodes())]\n","pos = nx.spring_layout(full_graph)\n","nx.draw(full_graph, pos=pos, arrows=True, node_size=10, node_color=colors, arrowstyle='-|>', arrowsize=5, width=0.2)\n","plt.savefig(\"graph.png\", dpi=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKNK4wsG-icW"},"outputs":[],"source":["rec = nx.reciprocity(full_graph)\n","print(f\"Reciprocity: {rec}\")"]},{"cell_type":"markdown","metadata":{"id":"JAIrgY1YF6oF"},"source":["# 📐 Applying Measures\n","\n","Meaning of measures:\n","\n","*   **Degree Centrality**, considering the entire population of users, the degree centrality is the ratio between the number of followers a user has with respect to the total population, excluding itself.\n","*   **Eigenvector Centrality (left)**, the user's centrality is given by the centrality of the users that follow it.\n","*   **Eigenvector Centrality (right)**, the user's centrality is given by the centrality of users it follows.\n","*   **Katz Centrality (left/rigth)**, more effective and solid formulation of Eigenvector Centrality as it reduces the impact a single high centrality has on all the users it follows.\n","*   **Closeness Centrality**, (Only applicable to single components!) the user's centrality is inversely proportional to the distance it has from all other nodes. When low it can act as a measure of how much a user is isolated.\n","*   **Betweenness Centrality**, centrality of a user depends on its position as a crossroad between paths from other users. It may measure how much a user acts as a \"common friend\" between others.\n","*   **Clustering Coefficient**, for each user u and the set of its neighbours Nu (users that follow it or are followed by it) this measure is the ratio between the number of couples of Nu that have a relationship between each other and their total number. This gives us insight on how a user acts as a centre of its local community."]},{"cell_type":"markdown","source":["## 1. Computing centrality measures (using NetworkIT and Networkx)"],"metadata":{"id":"3mAMFGgro8K0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJyvSufpGuH5"},"outputs":[],"source":["def list_to_dict_user(list_measure):\n","  dict_measure = {}\n","  for u in idmap.keys():\n","    dict_measure[u] = list_measure[idmap[u]]\n","  return dict_measure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34aXIzEa8GZN"},"outputs":[],"source":["full_graph_nk = nk.nxadapter.nx2nk(full_graph, data=True)\n","idmap = dict((id, u) for (id, u) in zip(full_graph.nodes(), range(full_graph.number_of_nodes())))"]},{"cell_type":"markdown","metadata":{"id":"5Yqb9JQLEK0H"},"source":["#### Degree Centrality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zw2YIa0CD3PS"},"outputs":[],"source":["deg_centr_nk = nk.centrality.DegreeCentrality(full_graph_nk, normalized=True).run().scores()"]},{"cell_type":"markdown","metadata":{"id":"kQ9NEcvhEUbP"},"source":["#### Eigenvector Centrality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inVcKwtPD2vb"},"outputs":[],"source":["eig_centr_nk = nk.centrality.EigenvectorCentrality(full_graph_nk).run().scores()"]},{"cell_type":"markdown","metadata":{"id":"Cs3XF-S9EX0n"},"source":["#### Katz Centrality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVHwMgrwD2ne"},"outputs":[],"source":["katz_centr_nk = nk.centrality.KatzCentrality(full_graph_nk, 0.001, 1e-4).run().scores()"]},{"cell_type":"markdown","metadata":{"id":"6GI-o62qEbFk"},"source":["#### Closeness Centrality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9u8aMR62D2fy"},"outputs":[],"source":["close_centr_nk = nk.centrality.Closeness(full_graph_nk, False, nk.centrality.ClosenessVariant.GENERALIZED).run().scores()"]},{"cell_type":"markdown","metadata":{"id":"Pjm6OkJdEeiy"},"source":["#### Betweennes Centrality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_No3T9BD2cE"},"outputs":[],"source":["betw_centr_nk = nk.centrality.Betweenness(full_graph_nk).run().scores()"]},{"cell_type":"markdown","metadata":{"id":"wo0srKDAEjLA"},"source":["#### Clustering Coefficient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3Lsd9WvD2Xo"},"outputs":[],"source":["clust = nx.clustering(full_graph)"]},{"cell_type":"markdown","source":["#### PageRank"],"metadata":{"id":"3X005xDDZa_S"}},{"cell_type":"code","source":["pagerank = nk.centrality.PageRank(full_graph_nk).run().scores()"],"metadata":{"id":"8qfexPJcZZsZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vzjrK0CZEnqE"},"source":["#### Hubs and Authorities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZAp1A_vEA_C"},"outputs":[],"source":["hits = nx.hits(full_graph)"]},{"cell_type":"markdown","metadata":{"id":"EQwS-xHSE6Ga"},"source":["#### Reputation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8h1Z1GmE420"},"outputs":[],"source":["full_in_degree = full_graph.in_degree\n","full_out_degree = full_graph.out_degree\n","reputation = {}\n","for node in full_graph.nodes().keys():\n","  reputation[node]=full_in_degree[node]/(full_in_degree[node]+full_out_degree[node]+1)"]},{"cell_type":"markdown","source":["#### Core Number"],"metadata":{"id":"o6Y0V0i9eCm3"}},{"cell_type":"code","source":["core_number = nx.core_number(full_graph)"],"metadata":{"id":"RdaRFQ2meE9_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### In/Out"],"metadata":{"id":"Qqi3Kl07feI9"}},{"cell_type":"code","source":["in_over_out = {}\n","for node in full_graph.nodes().keys():\n","  in_over_out[node]=(full_in_degree[node]+1)/((full_out_degree[node])+1)"],"metadata":{"id":"xsJI2ZcJfd9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### PageRank/Degree"],"metadata":{"id":"eS-pabPve5RK"}},{"cell_type":"code","source":["pagerank_over_degree = {}\n","pagerank = list_to_dict_user(pagerank)\n","for node in full_graph.nodes().keys():\n","  pagerank_over_degree[node]=pagerank[node]/(full_in_degree[node]+full_out_degree[node]+1)"],"metadata":{"id":"NBjg8RYme4_r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_posts = [full_graph.nodes[node]['N_posts'] for node in list(full_graph.nodes())]\n","print(len(list_to_dict_user(n_posts)))"],"metadata":{"id":"3M3Buzai78MC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMe6FrY9EGHR"},"outputs":[],"source":["measures = {\n","    'n_posts': list_to_dict_user(n_posts),\n","    'in_degree': dict(full_in_degree),\n","    'out_degree': dict(full_out_degree),\n","    'degree_centrality': list_to_dict_user(deg_centr_nk),\n","    #'eigenvector_centrality': list_to_dict_user(eig_centr_nk),\n","    #'katz_centrality': list_to_dict_user(katz_centr_nk),\n","    #'closeness_centrality': list_to_dict_user(close_centr_nk),\n","    #'betweenness_centrality': list_to_dict_user(betw_centr_nk),\n","    #'clustering_coefficient': clust,\n","    #'pagerank': pagerank,\n","    #'core_number': core_number,\n","    #'in_over_out': in_over_out,\n","    #'pagerank_over_degree': pagerank_over_degree,\n","    #'reputation': reputation,\n","    #'hubs': hits[0],\n","    #'authorities': hits[1],\n","}"]},{"cell_type":"markdown","metadata":{"id":"ayoK9ZOboa67"},"source":["## 2. Showing dataframe with measure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pvPwZa1FQL42"},"outputs":[],"source":["measure_df = []\n","for u in measure.keys():\n","    d = {\n","        'user_id' : u,\n","        'label' : label_dict[u],\n","        'measure' : measure[u]\n","    }\n","    measure_df.append(d)\n","\n","measure_df = pd.DataFrame(measure_df)\n","measure_df = measure_df.sort_values(by='measure', ascending=False)\n","display(measure_df.head(100))\n","top100_user_counts = measure_df[0:100]['label'].value_counts()\n","humans = top100_user_counts['human']\n","bots = top100_user_counts['bot']\n","tot = humans + bots\n","print(f\"Humans: {humans}\")\n","print(f\"Bots: {bots}\")\n","print(f\"Bot Percentage: {bots/tot}\")"]},{"cell_type":"markdown","metadata":{"id":"FJxFymrUohfj"},"source":["## 3. Plotting the graph with measures"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"IOQKu_e3Cv4i"},"outputs":[],"source":["plt.figure(figsize=(30,30))\n","cent = np.fromiter(measure.values(), float)\n","sizes = cent / np.max(cent) * 200\n","normalize = mcolors.Normalize(vmin=cent.min(), vmax=cent.max())\n","\n","pos = nx.spring_layout(full_graph)\n","colors = [colormap[full_graph.nodes[node]['label']] for node in list(full_graph.nodes())]\n","nx.draw(full_graph, pos, node_size=sizes, node_color=colors ,arrowstyle='-|>', arrows=True ,arrowsize=5, width=0.2)#node_color=sizes, cmap=colormap)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mO-5PzoYoqgS"},"source":["## 4. Visualizing the graph using Gravis"]},{"cell_type":"markdown","metadata":{"id":"5esfrNrJom6z"},"source":["### Rounding method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9T3HgDdeSz8j"},"outputs":[],"source":["def round_dict(dict):\n","  for key in dict.keys():\n","    dict[key] = round(dict[key], 4)\n","  return dict"]},{"cell_type":"markdown","metadata":{"id":"lvJeOABnh3Hs"},"source":["### Setting nodes attributes and exporting the graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qx1-Zn-xkPs"},"outputs":[],"source":["sizes = betw_centr.copy()\n","for k in betw_centr.keys():\n","  sizes[k] = 1 + betw_centr[k] * 1000\n","\n","for k in measures.keys():\n","  nx.set_node_attributes(full_graph, round_dict(measures[k]), k)\n","\n","\n","nx.set_node_attributes(full_graph, round_dict(sizes), 'size')\n","fig = gv.d3(full_graph)\n","fig.export_html('full_graph.html')"]},{"cell_type":"markdown","metadata":{"id":"BEcPKnWzoxAf"},"source":["## 5. Full graph, in and out degree plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEIcyVTjo0L-"},"outputs":[],"source":["full_in_degree = dict(full_graph.in_degree).values()\n","full_out_degree = dict(full_graph.out_degree).values()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A407EZ8ZjUE_"},"outputs":[],"source":["plt.hist(full_in_degree, bins=30, alpha=0.5, label='in-degree', range=(0,30))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSsQ4XCijRuZ"},"outputs":[],"source":["plt.hist(full_in_degree, bins=30, alpha=0.5, label='in-degree', range=(0,30))"]},{"cell_type":"markdown","metadata":{"id":"beiA7RUn2PAm"},"source":["# 🤖 Bot Network"]},{"cell_type":"markdown","metadata":{"id":"80bPuhcnpFZx"},"source":["## Bot Network Definition from nodes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9eI0cO2pEas"},"outputs":[],"source":["bot_users = [node for node in full_graph.nodes() if full_graph.nodes[node]['label'] == 'bot']\n","bot_graph = full_graph.subgraph(bot_users)"]},{"cell_type":"markdown","metadata":{"id":"fX0OLpBXreIE"},"source":["## Bot Network Definition from edges information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcfB6uaFriHt"},"outputs":[],"source":["bot_bot_edges = [edge for edge in full_graph.edges() if full_graph.edges[edge]['label'] == 'bot_bot']\n","\n","bot_bot_graph = full_graph.edge_subgraph(bot_bot_edges)\n","print(len(bot_bot_graph.edges()))"]},{"cell_type":"markdown","metadata":{"id":"UsHin2J-pNQg"},"source":["## Bot graph, in and out degree plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFQiRITWpQUH"},"outputs":[],"source":["bot_in_degree = dict(bot_graph.in_degree).values()\n","bot_out_degree = dict(bot_graph.out_degree).values()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yV8mxkrxZpF"},"outputs":[],"source":["plt.hist(bot_in_degree, bins=10, alpha=0.5, label='in-degree', range=(0,10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhP93kDJhufU"},"outputs":[],"source":["plt.hist(bot_out_degree, bins=10, alpha=0.5, label='out-degree', range=(0,10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eE7LkrfofYOC"},"outputs":[],"source":["colormap = {'human':'green', 'bot':'red'}\n","colors = [colormap[bot_graph.nodes[node]['label']] for node in list(bot_graph.nodes())]\n","nx.draw(bot_graph, arrows=True, node_size=10, node_color=colors, arrowstyle='-|>', arrowsize=5, width=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtR3P36W_GWZ"},"outputs":[],"source":["rec = nx.reciprocity(bot_graph)\n","print(f\"Bot Reciprocity: {rec}\")"]},{"cell_type":"markdown","metadata":{"id":"xogRMbfJ2X0s"},"source":["# 👥 Human Network"]},{"cell_type":"markdown","metadata":{"id":"-T4fv1j4pfAY"},"source":["## Human Network Definition from nodes"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"XfSyHQTX2YA1"},"outputs":[],"source":["human_users = [node for node in full_graph.nodes() if full_graph.nodes[node]['label'] == 'human']\n","human_graph = full_graph.subgraph(human_users)"]},{"cell_type":"markdown","metadata":{"id":"-33XNRehwOD1"},"source":["## Human Network Definition from edges information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uapLW8-jwRh0"},"outputs":[],"source":["human_human_edges = [edge for edge in full_graph.edges() if full_graph.edges[edge]['label'] == 'human_human']\n","\n","human_human_graph = full_graph.edge_subgraph(human_human_edges)\n","print(len(human_human_graph.edges()))"]},{"cell_type":"markdown","metadata":{"id":"FfGWa7Mippvx"},"source":["## Human graph, in and out degree plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btQ9p8vRppNe"},"outputs":[],"source":["human_in_degree = dict(human_graph.in_degree).values()\n","human_out_degree = dict(human_graph.out_degree).values()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9mskC1Zh8ag"},"outputs":[],"source":["plt.hist(human_in_degree, bins=25, alpha=0.5, label='in-degree', range=(0,25))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xMYuiIhh9pb"},"outputs":[],"source":["plt.hist(human_out_degree, bins=25, alpha=0.5, label='out-degree', range=(0,25))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_1rKWk4fcsr"},"outputs":[],"source":["plt.figure(figsize=(30,30))\n","colormap = {'human':'green', 'bot':'red'}\n","colors = [colormap[human_graph.nodes[node]['label']] for node in list(human_graph.nodes())]\n","nx.draw(human_graph, arrows=True, node_size=10, node_color=colors, arrowstyle='-|>', arrowsize=5, width=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifKRCZvI_K2T"},"outputs":[],"source":["rec = nx.reciprocity(human_graph)\n","print(f\"Human Reciprocity: {rec}\")"]},{"cell_type":"markdown","metadata":{"id":"7JE0UmlGwjHj"},"source":["# 👥🤖 Mixed (*human-bot* and *bot-human*) Network"]},{"cell_type":"markdown","metadata":{"id":"m8pXU9wfkh05"},"source":["## Mixed Network definition from edges information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in4-99JuwgYX"},"outputs":[],"source":["mixed_edges = [edge for edge in full_graph.edges() if full_graph.edges[edge]['label'] == 'human_bot' or full_graph.edges[edge]['label'] == 'bot_human']\n","\n","mixed_graph = full_graph.edge_subgraph(mixed_edges)\n","print(len(mixed_graph.edges()))"]},{"cell_type":"markdown","metadata":{"id":"whMKQJWtkdw0"},"source":["## Mixed Network reciprocity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMvkzUJ6xyS5"},"outputs":[],"source":["print(nx.reciprocity(mixed_graph))"]},{"cell_type":"markdown","metadata":{"id":"Xp1RciiMkM2T"},"source":["## Mixed graph, in and out degree plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2N264ndylc6"},"outputs":[],"source":["mixed_in_degree = dict(mixed_graph.in_degree).values()\n","mixed_out_degree = dict(mixed_graph.out_degree).values()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-C1OmLHynmz"},"outputs":[],"source":["plt.hist(mixed_in_degree, bins=25, alpha=0.5, label='in-degree', range=(0,25))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkl1sRhtyoQA"},"outputs":[],"source":["plt.hist(mixed_out_degree, bins=25, alpha=0.5, label='out-degree', range=(0,25))"]},{"cell_type":"markdown","metadata":{"id":"RrthpJar31Oc"},"source":["# 🧬 Measure Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AV3FAL5E4OIo"},"outputs":[],"source":["for m in measures.keys():\n","  corr = spy.stats.pointbiserialr(one_hot_label, list(measures[m].values()))\n","  print(f\"{m}: pvalue = {round(corr.pvalue,4)} --- statistic = {round(corr.statistic, 4)}\")"]},{"cell_type":"code","source":["one_hot_label = [1 if full_graph.nodes[node]['label']=='bot' else 0 for node in list(full_graph.nodes())]\n","measure_df = pd.DataFrame.from_dict(measures)\n","measure_df['label'] = one_hot_label"],"metadata":{"id":"Y7F6xldNJqz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_pL0UhA_zrh"},"outputs":[],"source":["pair = sns.pairplot(measure_df, hue='label')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3Fx-AyCCTI2"},"outputs":[],"source":["corr = measure_df.corr()\n","plt.figure(figsize=(15,10))\n","sns.heatmap(corr, cmap=\"YlGnBu\", annot=True);"]},{"cell_type":"markdown","source":["# ⚙️ Computing measures with multiprocessing"],"metadata":{"id":"lwep9Wg9_MJQ"}},{"cell_type":"markdown","source":["Usage of the multiprocessing module and the functions to compute the metrics from metrics.py (each functions is basically a wrapper of a networkit or networkx function to compute the specified metric, so that each function has the same args that we can use in the following methods). To add a new metric, add the wrap function into metrics.py and add the function name in the dictionary metric_functions below.\n","\n","The following code allows to build the graph, compute the measures and use them."],"metadata":{"id":"xhe8ePSV_SXq"}},{"cell_type":"markdown","source":["## Functions to create a graph based on the community name (useful to iterate measures and analysis over different communities)\n"],"metadata":{"id":"7RHlHVeAAMjg"}},{"cell_type":"code","source":["def create_df_network(df, users):\n","  return df[(df[\"source_id\"].isin(users)) & (df[\"target_id\"].isin(users))]"],"metadata":{"id":"7Hzpg93E-Bbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_community_df(community_name, user_hashtag_df):\n","  community = user_hashtag_df[user_hashtag_df['Hashtag'].str.contains(community_name, case=False)]['UserID'].to_list()\n","  #community = user_hashtag_df[user_hashtag_df['Hashtag'].str.lower()==community_name]['UserID'].to_list()\n","  df_list = []\n","  for k in range(8):\n","    chunk_edge_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/edge_chunks/edge_chunk_{k}.parquet\")\n","    df = create_df_network(chunk_edge_df, community)\n","    df_list.append(df)\n","\n","  final_df = pd.concat(df_list, ignore_index=True)\n","  return final_df"],"metadata":{"id":"1RJzUktxAQks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_following_network(final_df):\n","  following_df = final_df[final_df['relation']=='following']\n","  following_graph = nx.from_pandas_edgelist(following_df, 'source_id', 'target_id', create_using=nx.DiGraph())\n","  nx.set_node_attributes(following_graph, label_dict, 'label')\n","  nx.set_node_attributes(following_graph, color_dict, 'color')\n","  return following_graph"],"metadata":{"id":"OnoD3UlNASOx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_follower_network(final_df):\n","  followers_df = final_df[final_df['relation']=='followers']\n","  #followers_graph = nx.from_pandas_edgelist(followers_df, 'source_id', 'target_id', create_using=nx.DiGraph())\n","  followers_graph = nx.from_pandas_edgelist(followers_df, 'target_id', 'source_id', create_using=nx.DiGraph()) # Reversing the follower relationship, this way every arc in the graph goes from follower to followed.\n","  nx.set_node_attributes(followers_graph, label_dict, 'label')\n","  nx.set_node_attributes(followers_graph, color_dict, 'color')\n","  return followers_graph"],"metadata":{"id":"6CYjF8Q9AUJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_full_graph(community_name, user_hashtag_df):\n","  final_df = build_community_df(community_name, user_hashtag_df)\n","  full_graph = nx.compose(build_following_network(final_df), build_follower_network(final_df))\n","  nodes = full_graph.nodes()\n","  edge_dict = {}\n","  for e in full_graph.edges():\n","    start_label = nodes[e[0]]['label']\n","    end_label = nodes[e[1]]['label']\n","    edge_dict[e] = start_label + '_' + end_label\n","\n","  nx.set_edge_attributes(full_graph, edge_dict, 'edge_label')\n","  return full_graph"],"metadata":{"id":"UObjlTdwAVm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Functions to wrap the computations of measures with multiprocess to kill them after reaching a time limit"],"metadata":{"id":"eqX4K8N5AX8e"}},{"cell_type":"code","source":["def compute_reputation(full_graph):\n","  full_in_degree = full_graph.in_degree\n","  full_out_degree = full_graph.out_degree\n","  reputation = {}\n","  for node in full_graph.nodes().keys():\n","    reputation[node]=full_in_degree[node]/(full_in_degree[node]+full_out_degree[node])\n","  return reputation"],"metadata":{"id":"BpuGrYjyAZBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def list_to_dict_user(list_measure, idmap):\n","  dict_measure = {}\n","  for u in idmap.keys():\n","    dict_measure[u] = list_measure[idmap[u]]\n","  return dict_measure"],"metadata":{"id":"sascBksnAa6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The values of the following dict are the functions from metrics.py to compute the metrics."],"metadata":{"id":"lKFcxLUcAdU-"}},{"cell_type":"code","source":["metric_functions = {\n","    \"degree_centrality\": degree_centrality,\n","    \"eigenvector_centrality\": eigenvector_centrality,\n","    \"katz_centrality\": katz_centrality,\n","    \"closeness_centrality\": closeness_centrality,\n","    \"betweenness_centrality\": betweenness_centrality,\n","    \"clustering_coefficients\": clustering_coefficients,\n","    \"hits_scores\": hits_scores,\n","    \"reputation_score\": reputation_score\n","}"],"metadata":{"id":"V5j7ug-5AcT9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For each measure we use an async computation and then we save the result (if there is one) in the measure dict that we return in the end (special case for hubs and autorithies that come both from the same function)."],"metadata":{"id":"7XJwUk5qAgtx"}},{"cell_type":"code","source":["def compute_metrics(full_graph, time_limit=30):\n","    full_graph_nk = nk.nxadapter.nx2nk(full_graph, data=True)\n","    idmap = {id: u for id, u in zip(full_graph.nodes(), range(full_graph.number_of_nodes()))}\n","    measures = {}\n","    for name, func in metric_functions.items():\n","        print(f\"▶️ Computing {name}...\")\n","        with multiprocessing.Pool(processes=1) as pool:\n","          async_result = pool.apply_async(func, args=(full_graph, full_graph_nk, idmap))\n","          try:\n","            result = async_result.get(time_limit)\n","            if name == \"hits_scores\":\n","              measures[\"hubs\"] = result[0]\n","              measures[\"authorities\"] = result[1]\n","            else:\n","              measures[name] = result\n","            print(f\"✅ Done: {name}\")\n","          except multiprocessing.TimeoutError:\n","            print(f\"⏱️ Timeout after {time_limit}s\")\n","          finally:\n","            pool.terminate()\n","    return measures"],"metadata":{"id":"K5q-KB1uAfjB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📊 Testing different size Networks with different Detection Strategies\n","\n","n. |Task              |Small (<1.000) |Medium (1.000 - 10.000)|Large (>10.000)|\n","---|------------------|---------------|-----------------------|---------------|\n","1  |HITS              |  x            |  x                    |  x            |\n","2  |Extreme Behaviour |  x            |  x                    |  x            |\n","3  |Isolation Forest  |bot do not emerge as anomalies|bot do not emerge as anomalies|bot do not emerge as anomalies|\n","4  |DBSCAN            |  x            |  x                    |  x            |\n","5  |Logistic Regressor|  x            |  x                    |  x            |\n"],"metadata":{"id":"d7HmEK0tngG1"}},{"cell_type":"markdown","metadata":{"id":"4XjjLD8mJ4dm"},"source":["## Networks Definition\n","\n","\n"]},{"cell_type":"markdown","source":["*   **Small** $\\rightarrow$ Ruleoflaw, Feminist, Agenda2030\n","*   **Medium** $\\rightarrow$ Nato, Deeplearning, Nftcommunity\n","*   **Large** $\\rightarrow$ Ukraine, Ai, Covid"],"metadata":{"id":"YO9Tg9E6oJPJ"}},{"cell_type":"markdown","source":["## 1. HITS (Hubs & Authorities)"],"metadata":{"id":"F-ObEDcXMot_"}},{"cell_type":"markdown","source":["HITS, which stands for *Hyperlink-Induced Topic Search*, is an algorithm used to analyze the importance of nodes within a network, i.e. to identify authoritative and hub nodes in a network.\n","\n","- **Authorities:** nodes that are considered to be valuable sources of information.\n","\n","- **Hubs:** nodes that link to many authoritative nodes and help in discovering those authorities.\n","\n","Unlike centrality measures that focus on individual node properties, HITS considers the relationships between nodes to determine their authority and hub scores."],"metadata":{"id":"hg_-6wFyp63M"}},{"cell_type":"code","source":["def perform_hits_analysis(graph, label_dict, measures_dict, color_attribute='color', bot_color='red'):\n","    \"\"\"\n","    Performs HITS analysis on a network graph with weighted edges, iterating through a\n","    dictionary of measures and analyzing bots/humans based on a composite score for each measure.\n","    \"\"\"\n","\n","    for measure_name, measure_values in measures_dict.items():\n","        print(f\"\\n--- Analyzing with measure: {measure_name} ---\")\n","\n","        # Create a weighted graph using the current measure as weights\n","        weighted_graph = nx.DiGraph()\n","        for u, v, data in graph.edges(data=True):\n","            weight = measure_values.get(u, 1)  # default weight to 1 if measure not found\n","            weighted_graph.add_edge(u, v, weight=weight)\n","\n","        # Calculate HITS scores on the weighted graph\n","        hubs, authorities = nx.hits(weighted_graph, max_iter=1000, tol=1e-08, normalized=True)\n","\n","        # Create a composite score using the current measure\n","        composite_score = {}\n","        for node in graph.nodes():\n","            score = hubs[node] + authorities[node] + graph.nodes[node].get('betweenness_centrality', 0) + \\\n","                    graph.nodes[node].get('closeness_centrality', 0) + graph.nodes[node].get('clustering_coefficient', 0) + \\\n","                    graph.nodes[node].get('pagerank', 0)\n","            composite_score[node] = score\n","\n","        # Create a DataFrame with user IDs, labels, and HITS scores\n","        hits_df = pd.DataFrame([\n","            {\n","                'user_id': node,\n","                'label': label_dict.get(node),\n","                'hubs_score': hubs[node],\n","                'authorities_score': authorities[node],\n","                'composite_score': composite_score[node]\n","            }\n","            for node in graph.nodes()\n","        ])\n","\n","        # Sort in descending order and display top 100 users by composite score\n","        sorted_df = hits_df.sort_values(by='composite_score', ascending=False)\n","        print(\"Top 100 Users by Composite Score:\")\n","        print(sorted_df.head(100))\n","\n","        # Calculate percentage of bots in top users\n","        bot_percentage = (sorted_df.head(100)['label'] == 'bot').mean() * 100\n","        print(f\"\\nPercentage of bots in top 100 users: {bot_percentage:.2f}%\")\n","\n","        # Calculate percentage of humans in top users\n","        human_percentage = (sorted_df.head(100)['label'] == 'human').mean() * 100\n","        print(f\"\\nPercentage of humans in top 100 users: {human_percentage:.2f}%\")\n","\n","        # Plot distributions of HITS scores for bots and humans\n","        hits_scores = {'hubs': hubs, 'authorities': authorities}\n","        bot_nodes = [n for n, d in graph.nodes(data=True) if d.get(color_attribute) == bot_color]\n","        human_nodes = [n for n, d in graph.nodes(data=True) if d.get(color_attribute) != bot_color]\n","\n","        for score_type in hits_scores.keys():\n","            bot_scores = [hits_scores[score_type][n] for n in bot_nodes]\n","            human_scores = [hits_scores[score_type][n] for n in human_nodes]\n","            all_scores = bot_scores + human_scores\n","\n","            plt.figure(figsize=(10, 6))\n","            plt.title(f'Distribution of {score_type.capitalize()} Scores (Measure: {measure_name})')\n","            plt.hist(human_scores, bins=20, color='skyblue', alpha=0.7, label='Humans')\n","            plt.hist(bot_scores, bins=20, color='coral', alpha=0.7, label='Bots')\n","            plt.xlim(0, max(all_scores))\n","            plt.xlabel(score_type.capitalize() + ' Score')\n","            plt.ylabel('Frequency')\n","            plt.legend()\n","            plt.grid(True, linestyle='--', alpha=0.7)\n","            plt.tight_layout()\n","            plt.show()"],"metadata":{"id":"I1a9tNG9lyI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_hashtag_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/hashtag_users.parquet\")"],"metadata":{"id":"dyqFKVKwtCb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = pd.read_csv(f\"{SNA_PROJECT_PATH}/TwiBot-22/label.csv\")\n","labels.head()\n","colormap = {'human':'green', 'bot':'red'}\n","color_df = labels.replace(colormap)\n","label_dict = labels.set_index('id')['label'].to_dict()\n","color_dict = color_df.set_index('id')['label'].to_dict()"],"metadata":{"id":"seEDFg4Xt09k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["community = \"feminist\"\n","full_graph = build_full_graph(community, user_hashtag_df)"],"metadata":{"id":"2_9760KkuEkz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["measures_dict = {\n","    'degree_centrality': nx.degree_centrality(full_graph),\n","    'eigenvector_centrality': nx.eigenvector_centrality(full_graph, max_iter=1000),\n","    'katz_centrality': nx.katz_centrality(full_graph, alpha=0.1, beta=1.0),\n","    'closeness_centrality': nx.closeness_centrality(full_graph),\n","    'betweenness_centrality': nx.betweenness_centrality(full_graph),\n","    'clustering_coefficient': nx.clustering(full_graph),\n","    'reputation': compute_reputation(full_graph)\n","}"],"metadata":{"id":"6HeV3PrdyiFZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["perform_hits_analysis(full_graph, label_dict, measures_dict)"],"metadata":{"id":"fvFfVWC9wuPK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Looking for Extreme Behaviour in bot measures"],"metadata":{"id":"EhgDrXu3j37m"}},{"cell_type":"code","source":["# Creating a dictionary with all measures\n","measures_for_plot = {\n","    'degree_centrality': dict(),\n","    'eigenvector_centrality': dict(),\n","    'katz_centrality': dict(),\n","    'closeness_centrality': dict(),\n","    'betweenness_centrality': dict(),\n","    'clustering_coefficient': dict(),\n","    'reputation': dict(),\n","    'n_posts': dict()\n","}\n","botNodes = [x for x,y in full_graph.nodes(data=True) if y['color']=='red']"],"metadata":{"id":"2tOEwtTnkNIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for measure in measures_for_plot.keys():\n","  bot_dict = {}\n","  for bot in botNodes:\n","    bot_dict.update({bot : measures[measure][bot]})\n","\n","  human_dict = measures[measure].copy()\n","  for el in bot_dict:\n","    human_dict.pop(el)\n","\n","  bot_list = bot_dict.values()\n","  human_list = human_dict.values()\n","\n","  measures_for_plot[measure].update({'bots':bot_list})\n","  measures_for_plot[measure].update({'humans':human_list})"],"metadata":{"id":"rE8cfPFtkEpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for measure in measures_for_plot.keys():\n","  values = []\n","  for el in measures_for_plot[measure]['humans']:\n","    values.append(el)\n","  for el in measures_for_plot[measure]['bots']:\n","    values.append(el)\n","\n","  plt.figure(figsize=(10,10))\n","  plt.title(measure)\n","  plt.gca().get_yaxis().clear()\n","  plt.gca().get_xaxis().clear()\n","  plt.hist(measures_for_plot[measure]['humans'], alpha=0.8, label='humans')\n","  plt.xlim(0,max(values))\n","  plt.xlabel('humans')\n","  plt.show()\n","  plt.figure(figsize=(10,10))\n","  plt.title(measure)\n","  plt.gca().get_yaxis().clear()\n","  plt.gca().get_xaxis().clear()\n","  plt.hist(measures_for_plot[measure]['bots'], alpha=0.8, label='bots')\n","  plt.xlim(0,max(values))\n","  plt.xlabel('bots')\n","  plt.show()"],"metadata":{"id":"RlH1FRUQkGyF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. 4. 5. Machine Learning approaches to detect bots"],"metadata":{"id":"oxRvQoaWKQgV"}},{"cell_type":"code","source":["measure_df.head()"],"metadata":{"id":"1A09i7gcKQDf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Average number of posts by bots: {measure_df[measure_df['label']==1]['n_posts'].mean()}\")\n","print(f\"Average number of posts by humans: {measure_df[measure_df['label']==0]['n_posts'].mean()}\")"],"metadata":{"id":"CYua2VrH-WRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["measure_df_nl = measure_df.drop('label', axis=1)"],"metadata":{"id":"hpQ-FJwUKf6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scaling measures in order to compare them fairly\n","scaler = StandardScaler()\n","measure_df_scaled = scaler.fit_transform(measure_df_nl)\n","measure_df_nl.head()"],"metadata":{"id":"e0Cqwnz-K2bH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, x_test, y_train, y_test = train_test_split(measure_df_scaled, measure_df['label'], test_size=0.2, random_state=42)"],"metadata":{"id":"WbAeIs5gicHg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3) Isolation Forest"],"metadata":{"id":"sE-g1zqWthhL"}},{"cell_type":"code","source":["# Training model on normal behaviour\n","model = IsolationForest(contamination=0.1, n_estimators=100, max_samples=256, random_state=42)\n","model.fit(x_train)"],"metadata":{"id":"vd3iEgoDKyeW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","y_pred = model.predict(x_test)\n","y_pred = np.array([1 if y==-1 else 0 for y in y_pred])"],"metadata":{"id":"9WI_gh62jGIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# anomalies = measure_df[measure_df['pred_binary'] == 1]\n","# human_anomalies = anomalies[anomalies['label'] == 0]\n","# bot_anomalies = anomalies[anomalies['label'] == 1]\n","# print(f\"Avg human anomaly score among anomalies: {human_anomalies['anomaly_score'].mean()}\")\n","# print(f\"Avg bot anomaly score among anomalies: {bot_anomalies['anomaly_score'].mean()}\")"],"metadata":{"id":"bjctDXGTW8K6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# human_anomalies = measure_df[measure_df['label'] == 0]\n","# bot_anomalies = measure_df[measure_df['label'] == 1]\n","# print(f\"Avg human anomaly score: {human_anomalies['anomaly_score'].mean()}\")\n","# print(f\"Avg bot anomaly score: {bot_anomalies['anomaly_score'].mean()}\")"],"metadata":{"id":"NYLp847Ng8Rx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_test, y_pred))"],"metadata":{"id":"2zV_mhW8WKWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hm = sns.heatmap(confusion_matrix(y_test, y_pred, normalize='all'), annot=True)"],"metadata":{"id":"ackq9ASaMcPx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4) DBSCAN"],"metadata":{"id":"L5xsF6UQtmhL"}},{"cell_type":"code","source":["db = DBSCAN(eps=0.3, min_samples=100)\n","labels = db.fit_predict(measure_df_scaled)\n","measure_df['dbscan_label'] = labels"],"metadata":{"id":"MtXgGB-Gto-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15,10))\n","n = sns.countplot(x='dbscan_label', hue='label', data=measure_df)"],"metadata":{"id":"OGy3unJlt1lg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5) Logistic Regressor"],"metadata":{"id":"t4i0JgKRjAfi"}},{"cell_type":"code","source":["def prepare_data(full_graph, measures):\n","  x = []\n","  y = []\n","  for node in full_graph.nodes:\n","    label = 1 if full_graph.nodes[node]['label']=='bot' else 0\n","    features = []\n","    for m in measures:\n","      features.append(measures[m][node])\n","    x.append(features)\n","    y.append(label)\n","  return np.array(x), np.array(y)"],"metadata":{"id":"Pbq9BKQoi-zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_lr(full_graph, measures):\n","  x, y = prepare_data(full_graph, measures)\n","\n","  scaler = StandardScaler()\n","  x = scaler.fit_transform(x)\n","\n","  kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","  model = LogisticRegression(class_weight='balanced')\n","  model.fit(x, y)\n","  print(\"Cross-Validation Results:\\n\")\n","\n","  acc_scores = []\n","  all_y_true = []\n","  all_y_pred = []\n","\n","  coefficients_per_fold = []\n","\n","  for fold_idx, (train_index, test_index) in enumerate(kf.split(x, y)):\n","      X_train, X_test = x[train_index], x[test_index]\n","      y_train, y_test = y[train_index], y[test_index]\n","\n","      model.fit(X_train, y_train)\n","      y_pred = model.predict(X_test)\n","\n","      acc = accuracy_score(y_test, y_pred)\n","      acc_scores.append(acc)\n","\n","      all_y_true.extend(y_test)\n","      all_y_pred.extend(y_pred)\n","\n","      coefficients_per_fold.append(model.coef_[0])\n","\n","      print(f\"Fold {fold_idx + 1} Accuracy: {acc:.4f}\")\n","\n","  avg_coefficients = np.mean(coefficients_per_fold, axis=0)\n","\n","  print(\"\\nAverage Accuracy:\", np.mean(acc_scores))\n","  print(\"\\nClassification Report (Aggregated):\")\n","  print(classification_report(all_y_true, all_y_pred, target_names=[\"Human\", \"Bot\"]))\n","\n","  print(\"Confusion Matrix (Aggregated):\")\n","  print(confusion_matrix(all_y_true, all_y_pred))\n","\n","  print(\"\\nAverage Coefficients Across Folds:\")\n","  results = {}\n","  for feature, coef in zip(list(measures.keys()), avg_coefficients):\n","      results[feature] = coef\n","\n","  x_sm = sm.add_constant(x)\n","  model_sm = sm.Logit(y, x_sm)\n","  result = model_sm.fit(disp=0)\n","\n","  print(\"\\nCoefficient Significance (Full Dataset - statsmodels):\")\n","  summary_df = pd.DataFrame({\n","        \"Feature\": [\"Intercept\"] + list(measures.keys()),\n","        \"Coefficient\": result.params,\n","        \"P-value\": result.pvalues,\n","  })\n","  print(summary_df)\n","  return results, summary_df"],"metadata":{"id":"qM3jqUxfjIzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  coeff, df = test_lr(full_graph, measures)\n","  sorted_coeff = dict(sorted(coeff.items(), key=lambda item: item[1]))\n","  for c in sorted_coeff.keys():\n","    print(f\"  {c}: {sorted_coeff[c]:.4f}\")"],"metadata":{"id":"E4mQDPMmjRXT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5) Logistic Regressor w/ multiprocessing"],"metadata":{"id":"7phz_6B6xMLp"}},{"cell_type":"code","source":["user_hashtag_df = pd.read_parquet(f\"{SNA_PROJECT_PATH}/hashtag_users.parquet\")"],"metadata":{"id":"qSOUWnP5NEtE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = pd.read_csv(f\"{SNA_PROJECT_PATH}/TwiBot-22/label.csv\")\n","labels.head()\n","colormap = {'human':'green', 'bot':'red'}\n","color_df = labels.replace(colormap)\n","label_dict = labels.set_index('id')['label'].to_dict()\n","color_dict = color_df.set_index('id')['label'].to_dict()"],"metadata":{"id":"7mvlKGQ6PyFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["communities = [\"ukraine\", \"ai\", \"covid\"]"],"metadata":{"id":"xUOLLg28QCp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_limit = 10"],"metadata":{"id":"nup-rJk4f8nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for community_name in communities:\n","  full_graph = build_full_graph(community_name, user_hashtag_df)\n","  print(f\"Graph informations for {community_name} community: \", full_graph)\n","  print(f\"Computing measures for {community_name} community\")\n","  measures = compute_metrics(full_graph, time_limit)\n","  if len(measures) > 0:\n","    print(f\"LR results for {community_name}\")\n","    coeff, df = test_lr(full_graph, measures)\n","    sorted_coeff = dict(sorted(coeff.items(), key=lambda item: item[1]))\n","    for c in sorted_coeff.keys():\n","      print(f\"  {c}: {sorted_coeff[c]:.4f}\")"],"metadata":{"id":"pQQIXncEQNjU"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}